## urllib库介绍:相比requests不好用

### python内置的HTTP请求库

```
- urllib.request           请求模块
- urllib.error             异常处理模块
- urllib.parse             url解析模块
- urllib.robotpaser        robots.txt解析模块
```

### python2与python3的区别
#### python2

```
import urllib2
response = urllib2.urlopen('http://www.baidu.com')
```

#### python3

```
import urllib
response = urllib.resquest.('http://www.baidu.com')
```



urlopen

```
import urllib.requests

response = urllib.request.urlopen('http://www.baidu.com)
print(response.read(),decode('utf-8'))
```

```
import urllib.parse
import urllib.requests

data = bytes(urllib.parse.urlencode({'word':'hello'})), encoding = 'utf8')
response = urllib.request.urlopen('http://httpbin.org/post',data=data)
print(response.read())
```

```
import urllib.request

response = urllib.request.urlopen('http://httpbin.org/get', timeout=1)
print(response.read())
```

```
import socket
import urllib.request
import urllib.error

try:
    response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason,socket.timeout):
        print('TIME OUT')
```

### 响应

#### 响应类型

~~~
import urllib.request

response = urllib.request.urlopen('http://www.python.org')
print(type(response))
~~~

#### 状态码、响应头

```
import urllib.request

response = urllib.request.urlopen('http://www.python.org')
print(response.status)
print(response.getheaders())
print(response.getheader(server))
```

### request

```
# 使用requests获取网页信息，并把request当做对象传递给urlopen
import urllib.request

request = urllib.request.Request('http://www.baidu.com')
response = urllib.request.urlopen(request)
print(response.read(),decode('utf-8'))
```

~~~
from urllib import request,parse

url = 'http://httpbin.org/post'
headers = {'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.104 Safari/537.36 Core/1.53.3226.400 QQBrowser/9.6.11681.400'}
dict = {'name':'Gaterny'}
data = byte(parse.urlencode(dict),encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')
response = request.urlopen(req)
print(response.rad(),decode('utf-8'))
~~~

```
# 添加headers的另外方法

from urllib import request,parse

url = 'http://httpbin.org/post'
dict = {'name':'Gaterny'}
data = byte(parse.urlencode(dict),encoding='utf8')
req = request.Request(url=url, data=data, method='POST')
req.add_headers('User-Agent','Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.104 Safari/537.36 Core/1.53.3226.400 QQBrowser/9.6.11681.400')
response = request.urlopen(req)
print(response.rad(),decode('utf-8'))
```



### Handler

#### 代理

~~~
import urllib.request

proxy_handler = urllib.request.Proxy_Handler({'http://xxx.xx.xxx'})
opener = urllib.request.build_opener(proxy_handler)
response = opener.open('http://www.baidu.com')
print(response.read())
~~~

#### cookie

```
import http.cookiejar,urllib.request

cookie = htttp.cookiejar.Cookiejar()
handler = urllib.request.HTTPCookieProcess(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
for item in cookie:
    print(item.name+"="+item.value)
```

```
# 存取cookie文本文件

import http.cookiejar,urllib.request

filename = 'cookie.txt'
cookie = htttp.cookiejar.MozillaCookiejar(filename)
handler = urllib.request.HTTPCookieProcess(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
cookies.save(ignore_discard=True,ignore_expires=True)
```

```
# 读取cookie文件

import http.cookiejar,urllib.request

cookie = htttp.cookiejar.MozillaCookiejar(filename)
cookie.load('cookie.txt', ignore_discard=True,ignore_expires=True)
handler = urllib.request.HTTPCookieProcess(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open('http://www.baidu.com')
print(response.read(), decode('utf-8'))
```

### 异常处理

~~~
# URLError
from urllib import request.error

try:
    response = request.urlopen('http://t66y.com')
except(error.URLError as e):
    print(e.reason)
    
# HTTPError
from urllib import request.error

try:
    response = request.urlopen('http://t66y.com')
except(error.HTTPError as e):
    print(e.reason, e.code, e.headers,sep='\n')
except(error.URLError as e):
    print(e.reason)
else:
    print('Request Successfully!')
~~~

```
# 判断异常类型

import socket
import urllib.request
import urllib.error

try:
    response = request.urlopen('http://t66y.com', timeout=0.1)
except(error.URLError as e):
    print(type(e.reason))
    if isinstance(e.reason, socket.timeout)
        print('TIME OUT')
```



### URL解析

#### urlparse

```
urllib.parse.urlparse(urlstring,scheme='设置协议',allow_fragments=True)
```

```
# 分割url地址
from urllib.parse import urlparse

result = urlparse('http://www.baidu.com/index.html')
```

#### urlunparse

~~~
与urlparse相反，是将分割的协议拼接在一起
from urllib.parse import urlunparse

data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
print(urlunparse(data))
~~~

#### urljoin

```
from urllib.parse import urljoin

print('http://www.baidu.com', 'FAQ.html')
http://www.baidu.com/FAQ.html

# 前后域名以后面的域名为基准，前面的作为补充
```

#### urlencode

~~~
# 把字典对象转换成get请求参数

from urllib.parse import urlencode

params = {
'name':'Gaterny',
'age':23
}
baseurl = 'http://www.baidu.com?'
url = baseurl + urlencode(params)
print(url)

http://www.baidu.com?name=Gaterny&age=23
~~~

